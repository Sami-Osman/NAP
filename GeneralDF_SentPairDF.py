import os
from pickle import NONE
import re, string, nltk
import pandas as pd
from random import choice 
import numpy as np


# This class is used to create Argument pair from the data generated by Kialo_GeneralDF.py. 
# Uses ".pkl" dataset file in order to create a usable dataframe for [NLP] Next Sentence Prediction.

class argument_pair:
  def __init__(self, dataframe_path, dataFrame_name = "pair_dataset"):
    '''
      Parameters
        ----------
        dataframe_path: str
          The .pkl dataset file path that contains the all the sentence pool.
        dataFrame_name: str
          The name user chooses to give to the paired sentence dataset.
    '''

    self.dataframe_path = dataframe_path 
    self.df = pd.read_pickle(self.dataframe_path)
    self.dataFrame_name = dataFrame_name
    self.dataframe_argument_pairs = []
    self.flag = 0
    self.index = 0

  def pair_argu(self, argument1, position1, argument2, position2, idx):
    ''' 
        private method which gets pair of arguments from the argument pool, and asign label for NSP training.

        the below code puts Label to the data as 
        "YES NEXT argument = 0"
        "NOT NEXT argument = 1"

    Parameters
        ----------
        argument1: str
            first argument based on the current index from the [self.df] argument pool.
        argument2: str
            second argument based on the current index + 1 from the [self.df] argument pool.

        position1: int
            first argument position in the orignal kialo file with respect to its root argument.
            e.g [1.1] or [1.1.2] ...
        position2: int
            second argument position in the orignal kialo file with respect to its root argument.
            e.g [1.1] or [1.1.2] ...

        idx: int
            current traversed index from the [self.df] argument pool. [basically this is the index of argument1 in the pool]

    Returns
        -------
        pair_dataframe: list
            > The list of dictionary that contains each pair of Debate argument with assigned label as 0 or 1
        
    '''
    
    pos1 = position1
    pos2 = position2
    arg1 = argument1
    arg2 = argument2
    for label in [0, 1]:
      ''' Iterate over 0 and 1 and create a paired sentence, while 0 is "YES NEXT SENTENCE" and 1 is the opposite '''
      if(label == 1):
        if (self.flag == 0):
          ''' flag = 0 means create the pair by traversing the sentence position '''
          pos1 = position2
          pos2 = position1
          arg1 = argument2
          arg2 = argument1
          self.flag = 1
        else:
          ''' flag = 1 means create the pair by choosing the second sentence random from the pool, except it can not be the exact next sentence '''
          n = choice([i for i in range(0, len(self.df.index)) if i not in [idx+1]])
          pos1 = position1
          pos2 = self.df.position[n]
          arg1 = argument1
          arg2 = self.df.argument[n]
          self.flag = 0
      dataframe_arg = {
          "index": self.index,
          "label": label,
          "position1": pos1,
          "position2": pos2,
          "argument1": arg1,
          "argument2": arg2,
          
      }
      self.dataframe_argument_pairs.append(dataframe_arg)
      self.index +=1
  # text preprocessing helper functions
  def clean_text(self, text):
      '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
      and remove words containing numbers.'''
      text = text.lower()
      text = re.sub('\[.*?\]', '', text)
      text = re.sub('https?://\S+|www\.\S+', '', text)
      text = re.sub('<.*?>+', '', text)
      text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
      text = re.sub('\n', '', text)
      text = re.sub('\w*\d\w*', '', text)
      return text


  def text_preprocessing(self, text):
      """
      Cleaning and parsing the text.

      """
      tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
      nopunc = self.clean_text(text)
      tokenized_text = tokenizer.tokenize(nopunc)
      #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]
      combined_text = ' '.join(tokenized_text)
      return combined_text
      
  def Creat_Dataframe(self):

    '''

      Public method which gets content in the form of list and convert them to Panda DataFrame and store them on a pkl file.

        Parameters
        ----------
        content: list 
          The list of Debates extracted and indexed from a file.

        Returns
        -------
        dataframe: DataFrame
          > The DataFrame that contains each Debate argument with proper Futures ["index", "label", "position1", "position2", "argument1", "argument2"]
          > Saves the DataFrame in .pkl file for further use.

    '''

    folder = os.path.join(os.getcwd() , "dataframe")
    if not os.path.exists(folder):
      os.makedirs(folder)
    # transform the list of rows in a proper dataframe
    dataframe = pd.DataFrame(self.dataframe_argument_pairs)
    # Applying the cleaning function to datasets
    dataframe['argument1'] = dataframe['argument1'].apply(str).apply(lambda x: self.text_preprocessing(x))
    dataframe['argument2'] = dataframe['argument2'].apply(str).apply(lambda x: self.text_preprocessing(x))
    # Removing if either of the arguments are empty
    indx = dataframe[((dataframe.argument1 == ''))].index
    dataframe = dataframe.drop(indx, axis=0)
    indx = dataframe[((dataframe.argument2 == ''))].index
    dataframe = dataframe.drop(indx, axis=0)
    # resetting the DataFrame index
    dataframe = dataframe.reset_index(drop=True)

    dataframe = dataframe[list(dataframe.iloc[0].keys())]
    dataframe_path = os.path.join(folder + '/' + self.dataFrame_name + ".pkl")
    dataframe.to_pickle(dataframe_path)
    dataframe.to_excel(folder + '/' + self.dataFrame_name + '.xlsx', index = False)  
    return dataframe
    

  def pair_order(self):
    depth = []  # for tracking the back propagation while traversing the Kialo website tree data structure
    frst = 0
    scnd = 1
    for index in self.df.index:
      if(len(self.df.position[scnd]) == 1):
        frst = scnd
        scnd +=1
        depth = []
        continue
      else:
        if(len(self.df.position[scnd]) > len(self.df.position[frst])):
          depth.append(frst)
          pos1 = self.df.position[frst]
          argu1 = self.df.argument[frst]
          pos2 = self.df.position[scnd]
          argu2 = self.df.argument[scnd]
          self.pair_argu(argu1, pos1, argu2, pos2, index)
          frst = scnd
          scnd +=1 
        elif(len(self.df.position[scnd]) == len(self.df.position[frst])):
          frst = depth[-1]
          pos1 = self.df.position[frst]
          argu1 = self.df.argument[frst]
          pos2 = self.df.position[scnd]
          argu2 = self.df.argument[scnd]
          self.pair_argu(argu1, pos1, argu2, pos2, index)
          frst = scnd
          scnd +=1 
        else:
          frst = depth[-1]
          depth = depth[:-1] 
    return self.Creat_Dataframe()

def main(*arg):
  '''
    main method for GeneralDF_SentPairDF.py script:

    INPUT: Accepts from user
      > file Path
      > Accepts user prefered sentence pair Dataset name.

    OUTPUT: Argument pair DataFrame 
      > with ["index", "label", "position1", "position2", "argument1", "argument2"] Futures
  '''
  
  if(arg):
    path = arg[0]
    datasetName = arg[1]
  else:
    path = input('Inter File or folder path: ')
    datasetName = input("Give your Dataset a Name: ")
  Data = argument_pair(path, datasetName)
  pair_dataset = Data.pair_order()
  pair = pair_dataset.drop(['index', 'position1', 'position2'], axis = 1)
  print(datasetName, 'size: ', len(pair))
  print("\t 0 => [YES NEXT ARGUMENT] 1 => [NOT NEXT ARGUMENT]")
  print(pair['label'].value_counts())
  return pair

if __name__ == '__main__':
  main()
